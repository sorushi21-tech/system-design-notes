{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a74be1",
   "metadata": {},
   "source": [
    "# Caching Fundamentals\n",
    "\n",
    "## What is Caching?\n",
    "\n",
    "Caching is a technique used to store frequently accessed data in a fast storage layer (such as memory or a distributed cache) so that future requests for the same data can be served quickly, without repeatedly fetching it from a slower source like a database, file system, or external API.\n",
    "\n",
    "**Key points:**\n",
    "- Caching saves data closer to the application to improve speed and reduce load.\n",
    "- It acts as a buffer between the application and the slower data source.\n",
    "- Common cache types: in-memory (e.g., Redis, Memcached), browser cache, CDN edge cache.\n",
    "- Used in web apps, databases, operating systems, and more.\n",
    "\n",
    "\n",
    "**Caching minimize the**\n",
    "- Latency\n",
    "- Load on primary data source\n",
    "- Network calls\n",
    "- CPU and I/O usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a6629",
   "metadata": {},
   "source": [
    "## Why is Caching Needed?\n",
    "Caching is essential for building scalable, high-performance systems. It helps reduce latency, offload backend resources, and improve user experience.\n",
    "\n",
    "**Without caching:**\n",
    "- Every request hits the database or backend service\n",
    "- Higher response time for users\n",
    "- Increased CPU, memory, and database load\n",
    "- Poor scalability and higher infrastructure costs\n",
    "- Risk of bottlenecks and service degradation during peak loads\n",
    "- DB becomes a bottleneck\n",
    "- Without caching to handle larger load on db --> Horizantal scaling will be required\n",
    "- So ultimately it all increase the cloud cost\n",
    "\n",
    "\n",
    "**With caching:**\n",
    "- Faster responses (data served from memory or local storage)\n",
    "- Reduced database and backend calls\n",
    "- Better scalability and cost efficiency\n",
    "- Improved user experience and perceived performance\n",
    "- Ability to handle higher traffic with the same resources\n",
    "\n",
    "**Example:**\n",
    "A news website caches the homepage articles. Instead of querying the database for every user, it serves the cached version, reducing load and delivering content instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c8852",
   "metadata": {},
   "source": [
    "## How Does Cache Work?\n",
    "\n",
    "Applications typically store data in a database, which requires network calls and I/O operations—these are time-consuming and can slow down response times.\n",
    "\n",
    "A cache acts as a high-speed data layer between the application and the database. When a request is made:\n",
    "1. The application first checks the cache for the required data.\n",
    "2. If the data is found (cache hit), it is returned immediately, avoiding a database call.\n",
    "3. If the data is not found (cache miss), the application fetches it from the database, stores it in the cache for future requests, and then returns it to the user.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces network and database load\n",
    "- Improves API and application performance\n",
    "- Enables systems to scale efficiently under heavy traffic\n",
    "\n",
    "**Illustration:**\n",
    "- Without cache: Every request → Database → Slower response\n",
    "- With cache: Most requests → Cache → Fast response; only occasional requests go to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656db8a4",
   "metadata": {},
   "source": [
    "## Types of Caching \n",
    "\n",
    "### 1. In-Memory Cache\n",
    "- **Local (per-JVM):** Fastest access, data stored in the application's memory (e.g., Caffeine, Guava Cache). No network overhead, but not shared across nodes. Suitable for small datasets and stateless services.\n",
    "- **Distributed:** Data is stored in a cluster of cache servers (e.g., Redis, Memcached, Hazelcast, Apache Ignite). Enables horizontal scaling and high availability. Used for large-scale, multi-node Java applications.\n",
    "\n",
    "### 2. Persistent/Hybrid Cache\n",
    "- Combines memory and disk storage (e.g., Ehcache, Ignite). Retains data across restarts, useful for large datasets or when cache warm-up is expensive.\n",
    "\n",
    "### 3. CDN (Content Delivery Network) Cache\n",
    "- Caches static assets (images, JS, CSS) at edge locations close to users. Reduces latency and offloads backend servers. Not Java-specific, but critical for web-scale systems.\n",
    "\n",
    "### 4. Application-Level Cache\n",
    "- Caching at the service or repository layer (e.g., using Spring Cache abstraction). Can cache method results, database queries, or expensive computations. Supports annotations and AOP for declarative caching.\n",
    "\n",
    "### 5. Database Cache\n",
    "- **Query cache:** Caches results of frequent DB queries (e.g., MySQL Query Cache, Hibernate 2nd Level Cache).\n",
    "- **Row/Entity cache:** Caches individual rows/entities (e.g., JPA/Hibernate L2 cache, Redis as a backing store).\n",
    "- **Write-through/read-through:** Integrates cache with DB operations for consistency.\n",
    "\n",
    "### 6. HTTP Cache\n",
    "- Caches HTTP responses at the client, proxy, or gateway (e.g., using Cache-Control headers, reverse proxies like NGINX, or API gateways).\n",
    "- Useful for REST APIs and microservices to reduce repeated processing.\n",
    "\n",
    "### 7. OS/Page Cache\n",
    "- Operating system caches disk blocks in memory. Impacts DB and file I/O performance. Not directly controlled by Java, but important for backend performance tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031819a9",
   "metadata": {},
   "source": [
    "## Caching in Distributed Systems:\n",
    "\n",
    "### 1. Cache Consistency Models\n",
    "- **Strong Consistency:** Guarantees that all nodes see the same data at the same time. Achieved via distributed locks or consensus protocols (e.g., Zookeeper, etcd), but can impact performance.\n",
    "- **Eventual Consistency:** Updates propagate asynchronously; temporary staleness is tolerated. Most distributed caches (e.g., Redis, Memcached) use this model for scalability.\n",
    "- **Read-Your-Writes Consistency:** Guarantees that a client always sees its own updates, even if other clients see stale data.\n",
    "\n",
    "### 2. Cache Invalidation in Distributed Environments\n",
    "- **Explicit Invalidation:** Application sends a message/event to all cache nodes to remove or update a key (e.g., using pub/sub in Redis).\n",
    "- **Time-Based Expiry (TTL):** Each node independently expires data after a set time. Simple, but may serve stale data briefly.\n",
    "- **Versioning:** Store a version/timestamp with each cache entry; clients ignore stale versions.\n",
    "\n",
    "### 3. Preventing Cache Stampede and Avalanche\n",
    "- **Locking/Request Coalescing:** Only one thread fetches data from the DB on a cache miss; others wait for the result.\n",
    "- **Randomized TTLs:** Prevents many keys from expiring simultaneously, avoiding spikes in DB load.\n",
    "- **Background Refresh:** Proactively refresh hot keys before they expire.\n",
    "\n",
    "### 4. Monitoring and Observability\n",
    "- Track cache hit/miss rates, evictions, and latency across all nodes.\n",
    "- Use distributed tracing to correlate cache performance with end-to-end latency.\n",
    "- Integrate with monitoring tools (Prometheus, Grafana, ELK stack).\n",
    "\n",
    "### 5. Security and Data Protection\n",
    "- Avoid caching sensitive data unless encrypted and access-controlled.\n",
    "- Use network-level security (TLS, firewalls) for distributed cache clusters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c0379",
   "metadata": {},
   "source": [
    "## Why Consider Database Caching?\n",
    "\n",
    "Database caching is a critical technique for modern data architectures, especially in high-concurrency, low-latency environments. By storing frequently accessed data in memory, systems can dramatically reduce response times and backend load.\n",
    "\n",
    "### Key Benefits\n",
    "- **Reduced Latency:** Serving data from cache eliminates slow disk I/O and network round-trips to the database, resulting in sub-millisecond response times.\n",
    "- **Scalability:** Caching enables systems to handle thousands or millions of concurrent users without overwhelming the database.\n",
    "- **Cost Efficiency:** Reduces the need for expensive database scaling (vertical or horizontal), lowering infrastructure costs.\n",
    "- **Improved Throughput:** Batch jobs and analytics queries run significantly faster when intermediate results are cached.\n",
    "- **Resilience:** Caching can help absorb traffic spikes and protect the database from overload during peak usage.\n",
    "\n",
    "### Real-World Impact\n",
    "- Jobs that previously took hours can be reduced to minutes, and jobs that took minutes cut to seconds.\n",
    "- E-commerce sites cache product catalogs and inventory to deliver instant search results.\n",
    "- Social networks cache user profiles and feeds to support real-time interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107058cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
